{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Text summarization using PEGASUS\nIntroducing text summarization technique using PEGASUS which is powerful model for abstractive summarization. In PEGASUS, important sentences are removed/masked from an input txt such as BERT and are generated together as one output sequence from the remaining sentences, similar to an extractive summary.\n\nArXiv: https://arxiv.org/abs/1912.08777","metadata":{"id":"4vjMoDeWCZT7"}},{"cell_type":"markdown","source":"![](https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/f4061bd225b3be5b3f5b18eb1a229ce991efefeb/2-Figure1-1.png)","metadata":{"id":"NlOcokfoCZUK"}},{"cell_type":"code","source":"!pip install pyarrow>=6.1.0\n!pip install transformers\n!pip install sentencepiece","metadata":{"id":"3iNAE-rfCZUQ","outputId":"b8b9abbe-044d-43a6-eae6-f44a7f7c744f","execution":{"iopub.status.busy":"2022-09-03T08:41:52.990754Z","iopub.execute_input":"2022-09-03T08:41:52.991403Z","iopub.status.idle":"2022-09-03T08:42:17.099004Z","shell.execute_reply.started":"2022-09-03T08:41:52.991304Z","shell.execute_reply":"2022-09-03T08:42:17.097778Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2022-09-03T08:42:17.101009Z","iopub.execute_input":"2022-09-03T08:42:17.101582Z","iopub.status.idle":"2022-09-03T08:42:17.106693Z","shell.execute_reply.started":"2022-09-03T08:42:17.101532Z","shell.execute_reply":"2022-09-03T08:42:17.105975Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n","metadata":{"id":"-hf9NCrQCZUU","execution":{"iopub.status.busy":"2022-09-03T08:42:17.107781Z","iopub.execute_input":"2022-09-03T08:42:17.108510Z","iopub.status.idle":"2022-09-03T08:42:17.123165Z","shell.execute_reply.started":"2022-09-03T08:42:17.108469Z","shell.execute_reply":"2022-09-03T08:42:17.121019Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"../input/snerhack/train.csv\")\ndf.head()\n","metadata":{"id":"53IsWFWoCZUV","outputId":"a03ccd4f-f0f8-4410-bdc8-d91e1a419fb7","execution":{"iopub.status.busy":"2022-09-03T08:42:17.133445Z","iopub.execute_input":"2022-09-03T08:42:17.134373Z","iopub.status.idle":"2022-09-03T08:42:17.255709Z","shell.execute_reply.started":"2022-09-03T08:42:17.134332Z","shell.execute_reply":"2022-09-03T08:42:17.254972Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"n=15\ntx=df[\"Abstract\"][n]","metadata":{"id":"BvpYySzdCZUX","execution":{"iopub.status.busy":"2022-09-03T08:42:17.257143Z","iopub.execute_input":"2022-09-03T08:42:17.257619Z","iopub.status.idle":"2022-09-03T08:42:17.264437Z","shell.execute_reply.started":"2022-09-03T08:42:17.257584Z","shell.execute_reply":"2022-09-03T08:42:17.263609Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"tx","metadata":{"id":"LR6Ln9g5CZUa","outputId":"dc536305-b134-47fd-cefc-341ac1cf7ff9","execution":{"iopub.status.busy":"2022-09-03T08:42:17.265985Z","iopub.execute_input":"2022-09-03T08:42:17.266579Z","iopub.status.idle":"2022-09-03T08:42:17.276463Z","shell.execute_reply.started":"2022-09-03T08:42:17.266466Z","shell.execute_reply":"2022-09-03T08:42:17.275688Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# from transformers import PegasusForConditionalGeneration, AutoTokenizer\n# import torch\n\n# # You can chose models from following list\n# # https://huggingface.co/models?sort=downloads&search=google%2Fpegasus\n# model_name = 'google/pegasus-cnn_dailymail'\n# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n# model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n# batch = tokenizer(tx, truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n# translated = model.generate(**batch)\n# tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n\n# tgt_text","metadata":{"id":"McGXbrFsCZUc","outputId":"88c79845-149a-484f-8172-518969594837","execution":{"iopub.status.busy":"2022-09-03T08:42:17.278151Z","iopub.execute_input":"2022-09-03T08:42:17.278737Z","iopub.status.idle":"2022-09-03T08:42:17.286394Z","shell.execute_reply.started":"2022-09-03T08:42:17.278698Z","shell.execute_reply":"2022-09-03T08:42:17.285736Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# from rouge_score import rouge_scorer\n\n# scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n# scores = scorer.score(df[\"RHS\"][n],\n#                       tgt_text[0])","metadata":{"id":"ViM3GYAICZUe","execution":{"iopub.status.busy":"2022-09-03T08:42:17.287432Z","iopub.execute_input":"2022-09-03T08:42:17.288298Z","iopub.status.idle":"2022-09-03T08:42:17.296098Z","shell.execute_reply.started":"2022-09-03T08:42:17.288265Z","shell.execute_reply":"2022-09-03T08:42:17.295313Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# scores","metadata":{"id":"keMQ7MV-CZUf","execution":{"iopub.status.busy":"2022-09-03T08:42:17.298113Z","iopub.execute_input":"2022-09-03T08:42:17.298376Z","iopub.status.idle":"2022-09-03T08:42:17.305762Z","shell.execute_reply.started":"2022-09-03T08:42:17.298336Z","shell.execute_reply":"2022-09-03T08:42:17.304778Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\"\"\"Script for fine-tuning Pegasus\nExample usage:\n  # use XSum dataset as example, with first 1000 docs as training data\n  from datasets import load_dataset\n  dataset = load_dataset(\"xsum\")\n  train_texts, train_labels = dataset['train']['document'][:1000], dataset['train']['summary'][:1000]\n  \n  # use Pegasus Large model as base for fine-tuning\n  model_name = 'google/pegasus-large'\n  train_dataset, _, _, tokenizer = prepare_data(model_name, train_texts, train_labels)\n  trainer = prepare_fine_tuning(model_name, tokenizer, train_dataset)\n  trainer.train()\n \nReference:\n  https://huggingface.co/transformers/master/custom_datasets.html\n\"\"\"\n\nfrom transformers import PegasusForConditionalGeneration, PegasusTokenizer, Trainer, TrainingArguments\nimport torch\n\n\nclass PegasusDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels['input_ids'][idx])  # torch.tensor(self.labels[idx])\n        return item\n    def __len__(self):\n        return len(self.labels['input_ids'])  # len(self.labels)\n\n      \ndef prepare_data(model_name, \n                 train_texts, train_labels, \n                 val_texts=None, val_labels=None, \n                 test_texts=None, test_labels=None):\n    \"\"\"\n    Prepare input data for model fine-tuning\n    \"\"\"\n    tokenizer = PegasusTokenizer.from_pretrained(model_name)\n\n    prepare_val = False if val_texts is None or val_labels is None else True\n    prepare_test = False if test_texts is None or test_labels is None else True\n\n    def tokenize_data(texts, labels):\n        encodings = tokenizer(texts, truncation=True, padding=True)\n        decodings = tokenizer(labels, truncation=True, padding=True)\n        dataset_tokenized = PegasusDataset(encodings, decodings)\n        return dataset_tokenized\n\n    train_dataset = tokenize_data(train_texts, train_labels)\n    val_dataset = tokenize_data(val_texts, val_labels) if prepare_val else None\n    test_dataset = tokenize_data(test_texts, test_labels) if prepare_test else None\n\n    return train_dataset, val_dataset, test_dataset, tokenizer","metadata":{"id":"ZLNK8HqJCZUi","execution":{"iopub.status.busy":"2022-09-03T08:42:17.308316Z","iopub.execute_input":"2022-09-03T08:42:17.308596Z","iopub.status.idle":"2022-09-03T08:42:25.917761Z","shell.execute_reply.started":"2022-09-03T08:42:17.308557Z","shell.execute_reply":"2022-09-03T08:42:25.917034Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def prepare_fine_tuning(model_name, tokenizer, train_dataset, val_dataset=None, freeze_encoder=False, output_dir='./results'):\n    \"\"\"\n    Prepare configurations and base model for fine-tuning\n    \"\"\"\n    torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n\n    if freeze_encoder:\n        for param in model.model.encoder.parameters():\n            param.requires_grad = False\n\n    if val_dataset is not None:\n        training_args = TrainingArguments(\n        output_dir=output_dir,           # output directory\n        num_train_epochs=1,           # total number of training epochs\n        per_device_train_batch_size=2,   # batch size per device during training, can increase if memory allows\n        per_device_eval_batch_size=2,    # batch size for evaluation, can increase if memory allows\n        save_steps=100,                  # number of updates steps before checkpoint saves\n        save_total_limit=2,              # limit the total amount of checkpoints and deletes the older checkpoints\n        evaluation_strategy='steps',     # evaluation strategy to adopt during training\n        eval_steps=100,                  # number of update steps before evaluation\n        warmup_steps=200,                # number of warmup steps for learning rate scheduler\n        weight_decay=0.01,               # strength of weight decay\n        logging_dir='./logs',            # directory for storing logs\n        logging_steps=10,\n     )\n\n        trainer = Trainer(\n          model=model,                         # the instantiated 🤗 Transformers model to be trained\n          args=training_args,                  # training arguments, defined above\n          train_dataset=train_dataset,         # training dataset\n          eval_dataset=val_dataset,            # evaluation dataset\n          tokenizer=tokenizer\n        )\n\n    else:\n        training_args = TrainingArguments(\n          output_dir=output_dir,           # output directory\n          num_train_epochs=1,           # total number of training epochs\n          per_device_train_batch_size=2,   # batch size per device during training, can increase if memory allows\n          save_steps=100,                  # number of updates steps before checkpoint saves\n          save_total_limit=2,              # limit the total amount of checkpoints and deletes the older checkpoints\n          warmup_steps=200,                # number of warmup steps for learning rate scheduler\n          weight_decay=0.01,               # strength of weight decay\n          logging_dir='./logs',            # directory for storing logs\n          logging_steps=10,\n        )\n\n        trainer = Trainer(\n          model=model,                         # the instantiated 🤗 Transformers model to be trained\n          args=training_args,                  # training arguments, defined above\n          train_dataset=train_dataset,         # training dataset\n          tokenizer=tokenizer\n        )\n\n    return trainer\n\n\nif __name__=='__main__':\n  # use XSum dataset as example, with first 1000 docs as training data\n    train_texts, train_labels = list(df['Abstract'][:1000].values), list(df['RHS'][:1000].values)\n  \n  # use Pegasus Large model as base for fine-tuning\n    model_name = 'google/pegasus-large'\n    train_dataset, _, _, tokenizer = prepare_data(model_name, train_texts, train_labels)\n    trainer = prepare_fine_tuning(model_name, tokenizer, train_dataset)\n    trainer.train()","metadata":{"id":"QcumvkUWCZUl","outputId":"9b864aa1-785b-4218-c272-9430287acfb2","execution":{"iopub.status.busy":"2022-09-03T08:42:25.919167Z","iopub.execute_input":"2022-09-03T08:42:25.919434Z","iopub.status.idle":"2022-09-03T08:52:48.063006Z","shell.execute_reply.started":"2022-09-03T08:42:25.919399Z","shell.execute_reply":"2022-09-03T08:52:48.062025Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from transformers import PegasusForConditionalGeneration, PegasusTokenizer, Trainer, TrainingArguments\nimport torch\n\n# You can chose models from following list\n# https://huggingface.co/models?sort=downloads&search=google%2Fpegasus\nmodel_name = './results/checkpoint-500'\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\nbatch = tokenizer(tx, truncation=True, padding='longest', return_tensors=\"pt\").to(device)\ntranslated = model.generate(**batch)\ntgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n\ntgt_text","metadata":{"id":"A64gQpI9E9SU","execution":{"iopub.status.busy":"2022-09-03T09:35:06.732663Z","iopub.execute_input":"2022-09-03T09:35:06.733035Z","iopub.status.idle":"2022-09-03T09:35:21.553522Z","shell.execute_reply.started":"2022-09-03T09:35:06.733002Z","shell.execute_reply":"2022-09-03T09:35:21.552759Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}